# scripts/15_xgb_filtered_train.py
# ì¤‘ìš”í•œ feature ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµí–ˆì„ ë•Œ ê²°ê³¼

import pandas as pd
import joblib
from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error
from math import sqrt

# ìœ ì§€í•  ì£¼ìš” featureë§Œ ì„ ì • (SHAP ê¸°ë°˜)
selected_features = [
    'a_mean', 'area', 'b_mean', 'homogeneity',
    'energy', 'S_mean'  # ì˜í–¥ë ¥ ë†’ê³  ë°©í–¥ì„± ëšœë ·í–ˆë˜ featureë“¤
]

# ë°ì´í„° ë¡œë“œ ë° í•„í„°ë§
df = pd.read_csv('features/apple_features.csv')
X = df[selected_features]
y = df['SSC']

# ë°ì´í„° ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ëª¨ë¸ í•™ìŠµ
model = XGBRegressor(n_estimators=100, max_depth=6, learning_rate=0.1, random_state=42)
model.fit(X_train, y_train)

# ì˜ˆì¸¡ ë° í‰ê°€
y_pred = model.predict(X_test)
r2 = r2_score(y_test, y_pred)
rmse = sqrt(mean_squared_error(y_test, y_pred))

print("\nâœ… ì¤‘ìš” feature ê¸°ë°˜ XGBoost ì¬í•™ìŠµ ì™„ë£Œ")
print(f"R^2 Score: {r2:.4f}")
print(f"RMSE: {rmse:.4f}")

# ëª¨ë¸ ì €ì¥
joblib.dump(model, 'models/ssc_xgb_filtered.pkl')
print("\nğŸ“¦ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: models/ssc_xgb_filtered.pkl")
