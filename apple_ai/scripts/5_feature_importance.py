# scripts/5_feature_importance_and_retrain.py
# ëª¨ë¸ ë§Œë“¤ë•Œ ê°€ì¥ ì˜í–¥ì´ ë†’ì€ ìš”ì¸ë“¤ ì°¾ëŠ” ì½”ë“œ

import joblib
import pandas as pd
import matplotlib.pyplot as plt
from math import sqrt
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error
from xgboost import XGBRegressor

# ëª¨ë¸ ë° ë°ì´í„° ë¡œë”©
model = joblib.load('models/ssc_xgb_model.pkl')
df = pd.read_csv('features/apple_features.csv')

# ì…ë ¥ X, ë¼ë²¨ y ë¶„ë¦¬
X = df.drop(columns=['filename', 'grade', 'SSC'])
y = df['SSC']

# 1. Feature ì¤‘ìš”ë„ ê³„ì‚°
importances = model.feature_importances_
features = X.columns
importance_df = pd.DataFrame({'feature': features, 'importance': importances})
importance_df = importance_df.sort_values(by='importance', ascending=False)

# 2. ìƒìœ„ Nê°œ feature ì„ íƒ (ì˜ˆ: 8ê°œ)
TOP_N = 5
top_features = importance_df.head(TOP_N)['feature'].tolist()
print(f"\nğŸ¯ ìƒìœ„ {TOP_N}ê°œ feature: {top_features}")

# 3. ì„ íƒëœ featureë§Œìœ¼ë¡œ ì¬í•™ìŠµ
X_top = X[top_features]
X_train, X_test, y_train, y_test = train_test_split(X_top, y, test_size=0.2, random_state=42)

model_top = XGBRegressor(n_estimators=100, max_depth=6, learning_rate=0.1, random_state=42)
model_top.fit(X_train, y_train)

# 4. ì„±ëŠ¥ í‰ê°€
y_pred = model_top.predict(X_test)
r2 = r2_score(y_test, y_pred)
rmse = sqrt(mean_squared_error(y_test, y_pred))

print(f"\nğŸ“ˆ ì¬í•™ìŠµ ì„±ëŠ¥")
print(f"RÂ² Score: {r2:.4f}")
print(f"RMSE: {rmse:.4f}")

# 5. ì‹œê°í™”
importance_df.head(10).plot(kind='barh', x='feature', y='importance', legend=False)
plt.title("XGBoost Feature Importance (Top 10)")
plt.xlabel("Importance")
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

# 6. ëª¨ë¸ ì €ì¥ (ì„ íƒ)
joblib.dump(model_top, 'models/ssc_xgb_model_top8.pkl')
print("\nâœ… ì¤‘ìš” feature ê¸°ë°˜ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: models/ssc_xgb_model_top8.pkl")
